{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jazmi\\AppData\\Local\\Temp\\ipykernel_23476\\3595161705.py:17: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv('train_preprocess.tsv.txt',sep='\\t', error_bad_lines=False, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SELESAI\n",
      "Testing selesai\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.79      0.78       875\n",
      "     neutral       0.79      0.66      0.72       287\n",
      "    positive       0.88      0.90      0.89      1588\n",
      "\n",
      "    accuracy                           0.84      2750\n",
      "   macro avg       0.81      0.78      0.80      2750\n",
      "weighted avg       0.84      0.84      0.84      2750\n",
      "\n",
      "Sentimen:\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nlp_id.lemmatizer import Lemmatizer\n",
    "import pickle\n",
    "import sqlite3\n",
    "\n",
    "# Mengatur opsi pandas untuk menampilkan seluruh konten kolom\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Membaca data dari file CSV\n",
    "df = pd.read_csv('train_preprocess.tsv.txt',sep='\\t', error_bad_lines=False, header=None)\n",
    "df.rename(columns={0:'kalimat',1:'sentimen'}, inplace = True)\n",
    "\n",
    "def cleansing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\w+[0-9]\\w+',' ',text)\n",
    "    text = re.sub('__\\w+__',' ',text)\n",
    "    text = re.sub(r'[^A-Za-z0-9]',' ',text)\n",
    "    text = re.sub('( ){2,10}',' ',text)\n",
    "    return text\n",
    "\n",
    "# Membersihkan teks\n",
    "df['text_clean'] = df.kalimat.apply(cleansing)\n",
    "\n",
    "conn = sqlite3.connect('database_hate.db')\n",
    "call_alay = pd.read_sql_query('SELECT * FROM kamus_alay',conn)\n",
    "\n",
    "alay = dict(zip(call_alay['kata_alay'],call_alay['kata_normal']))\n",
    "\n",
    "#fungsi untuk mengganti kata alay ke normal\n",
    "def normalize(text):\n",
    "    hasil = []\n",
    "    splitting = text.split(' ')\n",
    "    for kata in splitting:\n",
    "        if kata in alay:\n",
    "            hasil.append(alay[kata])\n",
    "        else:\n",
    "            hasil.append(kata)\n",
    "    \n",
    "    return ' '.join(hasil)\n",
    "\n",
    "df['normal'] = df.text_clean.apply(normalize)\n",
    "\n",
    "\n",
    "# Menginisialisasi lemmatizer\n",
    "lema = Lemmatizer()\n",
    "\n",
    "# def lemas(texts):\n",
    "#     preprocess = [lema.lemmatize(text) for text in texts]\n",
    "#     return preprocess\n",
    "\n",
    "\n",
    "def lemas(text):\n",
    "    text = lema.lemmatize(text)\n",
    "    return text\n",
    "\n",
    "df['lemas'] = df.normal.apply(lemas)\n",
    "# Lematisasi teks\n",
    "#texts = df.normal.tolist()\n",
    "#df['lemas'] = lemas(texts)\n",
    "\n",
    "# Inisialisasi vektorisasi teks\n",
    "vect = TfidfVectorizer()\n",
    "fitur = vect.fit_transform(df.lemas)\n",
    "\n",
    "kata = vect.get_feature_names_out()\n",
    "\n",
    "# Membentuk dataframe dengan matriks fitur\n",
    "matrix = pd.DataFrame(fitur.toarray(), columns=kata)\n",
    "#pickle.dump(vect,open('fitur_5.p','wb'))\n",
    "\n",
    "# Memisahkan data menjadi data latih dan data uji\n",
    "target = df.sentimen\n",
    "X_train, X_test, y_train, y_test = train_test_split(fitur, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Melatih model\n",
    "model = MLPClassifier(random_state = 42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#pickle.dump(model.fit(X_train, y_train),open('model_fit.p','wb'))\n",
    "print('TRAINING SELESAI')\n",
    "\n",
    "\n",
    "# Mengevaluasi model\n",
    "test = model.predict(X_test)\n",
    "print(\"Testing selesai\")\n",
    "print(classification_report(y_test, test))\n",
    "#pickle.dump(model,open('model_5.p','wb'))\n",
    "# Melakukan prediksi pada contoh teks baru\n",
    "contoh = \"kamu keren banget!\"\n",
    "contoh1 = vect.transform([contoh])\n",
    "result = model.predict(contoh1)[0]\n",
    "print(\"Sentimen:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vect,open('fitur_fix.p','wb'))\n",
    "pickle.dump(model,open('model_fix.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
